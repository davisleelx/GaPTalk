import os, sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '../Deep3DFaceRecon_pytorch'))
import torch
import torch.nn.functional as F
import numpy as np
# from NetWorks.triplane import TriPlaneGenerator
# from NetWorks.dual_discriminator import DualDiscriminator
# from HeadNeRFOptions import BaseOptions
from Utils.HeadNeRFLossUtils import HeadNeRFLossUtils
# from Utils.RenderUtils import RenderUtils
from Utils.load_model import load_state_dict
import cv2
import time
from glob import glob
from tqdm import tqdm, trange
import imageio
import random
import argparse
from tool_funcs import put_text_alignmentcenter
from data import DataLoader
from omegaconf import OmegaConf
from Deep3DFaceRecon_pytorch.options.test_options import TestOptions
# from Deep3DFaceRecon_pytorch.models.networks import define_net_recon
from NetWorks.migan.lib.model_zoo.migan import Generator, Discriminator
from NetWorks import networks_modified as networks
from arguments import ModelParams, PipelineParams
from scene import Scene, GaussianModel, DeformModel
from scene.cameras import Camera
from gaussian_renderer import render

os.environ['TORCH_HOME'] = '/data/huangricong/commonModels/torch'

class FittingImage(object):
    def __init__(self, dataset, pipe, opt, gpu_id) -> None:
        super().__init__()

        self.device = torch.device("cuda:%d" % gpu_id)
        opt.opt_cam = False # True
        opt.view_num = 45
        opt.duration = 3.0 / opt.view_num
        if opt.model_path is not None:
            self.model_name = os.path.basename(opt.model_path)[:-4]

        self.opt = opt

        # para_dict = None
        # if self.opt.model_path is not None:
        #     check_dict = torch.load(self.opt.model_path, map_location=torch.device("cpu"))
        #     if "para" in check_dict.keys():
        #         para_dict = check_dict["para"]
        
        # self.opt.update(vars(BaseOptions(para_dict)))

        self.featmap_size = self.opt.featmap_size
        self.pred_img_size = self.opt.pred_img_size
        
        if not os.path.exists(self.opt.save_root):
            os.makedirs(self.opt.save_root)

        # self.netG = Generator(opt.netG.encoder, opt.netG.synthesis).to(self.device)
        # self.netD = Discriminator(**opt.netD).to(self.device)
        
        self.netG = networks.define_G(opt=opt, **opt.netG).to(self.device)
        self.netD = networks.define_D(**opt.netD).to(self.device)

        # if self.opt.model_path is not None:
        #     check_dict = torch.load(self.opt.model_path, map_location=torch.device("cpu"))
        #     load_state_dict(self.netG, check_dict['net'], replace='netG.')
        #     check_dict = torch.load(self.opt.model_path.replace('epoch', 'epoch_D'), map_location=torch.device("cpu"))
        #     load_state_dict(self.netD, check_dict['net'])

        self.loss_utils = HeadNeRFLossUtils(self.opt, device=self.device)
        # self.render_utils = RenderUtils(view_num=45, device=self.device, opt=self.opt)

        self.gaussians = GaussianModel(dataset.sh_degree)
        self.deform = DeformModel(dataset.is_blender, dataset.is_6dof)
        name = "epoch"
        self.deform.load_weights(dataset.model_path, name=name)
        scene = Scene(dataset, self.gaussians, load_iteration=-1, name=name)
        self.pipe = pipe

    def load_data(self, batch):
        self.rest_img = batch['rest_img'].to(self.device)
        # self.rest_img_full = batch['rest_img_full'].to(self.device)

        # self.img_full_tensor = batch['img_full'].to(self.device)

        self.mask_tensor = batch['mask'].to(self.device)
        self.img_crop_tensor = batch['img_crop'].to(self.device)
        self.img_crop_mask_tensor = batch['img_crop_mask'].to(self.device)
        
        self.rest_img = self.rest_img * 2. - 1.
        # self.rest_img_full = self.rest_img_full * 2. - 1.
        self.img_crop_tensor = self.img_crop_tensor * 2. - 1.
        # self.img_full_tensor = self.img_full_tensor * 2. - 1.
        self.img_crop_mask_tensor = self.img_crop_mask_tensor * 2. - 1.

        # load init codes from the results generated by solving 3DMM rendering opt.
        base_code = batch['base_code'].to(self.device)
        
        # self.base_iden = base_code[:, :self.opt.iden_code_dims]
        # self.base_expr = base_code[:, :, self.opt.iden_code_dims:self.opt.iden_code_dims + self.opt.expr_code_dims]
        # self.base_text = base_code[:, :, self.opt.iden_code_dims + self.opt.expr_code_dims:self.opt.iden_code_dims 
        #                                                     + self.opt.expr_code_dims + self.opt.text_code_dims]
        # self.base_illu = base_code[:, :, self.opt.iden_code_dims + self.opt.expr_code_dims + self.opt.text_code_dims:
        #     self.opt.iden_code_dims + self.opt.expr_code_dims + self.opt.text_code_dims + self.opt.illu_code_dims]
        
        # self.cam_info = {
        #     "batch_Rmats": batch['cam_info']['batch_Rmats'].to(self.device),
        #     "batch_Tvecs": batch['cam_info']['batch_Tvecs'].to(self.device),
        #     "batch_inmats": batch['cam_info']['batch_inmats'].to(self.device)
        # }
        # self.idx = batch['idx']
        # self.trans_params = batch['trans_params']
        # self.iden = batch['iden'].to(self.device)
        
        self.rest_mask = batch['rest_mask'].to(self.device)
        self.lm68 = batch['lm68'].to(self.device)
        # vertice = batch['vertice'].to(self.device)
        # self.vertice = vertice.reshape(vertice.shape[0], vertice.shape[1], -1)

    @staticmethod
    def eulurangle2Rmat(angles):
        batch_size = angles.size(0)
        
        sinx = torch.sin(angles[:, 0])
        siny = torch.sin(angles[:, 1])
        sinz = torch.sin(angles[:, 2])
        cosx = torch.cos(angles[:, 0])
        cosy = torch.cos(angles[:, 1])
        cosz = torch.cos(angles[:, 2])

        rotXs = torch.eye(3, device=angles.device).view(1, 3, 3).repeat(batch_size, 1, 1)
        rotYs = rotXs.clone()
        rotZs = rotXs.clone()
        
        rotXs[:, 1, 1] = cosx
        rotXs[:, 1, 2] = -sinx
        rotXs[:, 2, 1] = sinx
        rotXs[:, 2, 2] = cosx
        
        rotYs[:, 0, 0] = cosy
        rotYs[:, 0, 2] = siny
        rotYs[:, 2, 0] = -siny
        rotYs[:, 2, 2] = cosy

        rotZs[:, 0, 0] = cosz
        rotZs[:, 0, 1] = -sinz
        rotZs[:, 1, 0] = sinz
        rotZs[:, 1, 1] = cosz
        
        res = rotZs.bmm(rotYs.bmm(rotXs))
        return res
    
    
    def build_code_and_cam(self):
        # code
        # shape_code = torch.cat([self.base_iden + self.iden_offset[self.idx], self.base_expr + self.expr_offset[self.idx]], dim=-1)
        # shape_code = torch.cat([self.iden, self.base_expr + self.expr_offset[self.idx]], dim=-1)
        # appea_code = torch.cat([self.base_text, self.base_illu], dim=-1) + self.appea_offset[self.idx]

        shape_code = torch.cat([self.iden.unsqueeze(1).expand(-1, self.opt.clip_length, -1), self.base_expr], dim=-1)
        # shape_code = torch.cat([self.iden.unsqueeze(1).expand(-1, self.opt.clip_length, -1), self.vertice], dim=-1)
        appea_code = torch.cat([self.base_text, self.base_illu], dim=-1)
        
        opt_code_dict = {
            "bg": None,
            # "iden": self.iden_offset[self.idx],
            # "expr": self.expr_offset[self.idx],
            # "appea": self.appea_offset[self.idx],
            "expr": self.base_expr,
            "appea": torch.cat([self.base_text, self.base_illu], dim=-1),
        }
        
        code_info = {
            "bg_code": None, 
            "shape_code": shape_code, 
            "appea_code": appea_code, 
        }

        # cam
        if self.opt.opt_cam:
            delta_cam_info = {
                "delta_eulur": self.delta_EulurAngles[self.idx], 
                "delta_tvec": self.delta_Tvecs[self.idx],
            }

            batch_delta_Rmats = self.eulurangle2Rmat(self.delta_EulurAngles[self.idx])
            base_Rmats = self.cam_info["batch_Rmats"]
            base_Tvecs = self.cam_info["batch_Tvecs"]
            
            cur_Rmats = batch_delta_Rmats.bmm(base_Rmats)
            cur_Tvecs = batch_delta_Rmats.bmm(base_Tvecs) + self.delta_Tvecs[self.idx]
            
            batch_inmat = self.cam_info["batch_inmats"] # [N, 3, 3]
            batch_cam_info = {
                "batch_Rmats": cur_Rmats,
                "batch_Tvecs": cur_Tvecs,
                "batch_inmats": batch_inmat
            }
            
        else:
            delta_cam_info = None
            batch_cam_info = self.cam_info

        return code_info, opt_code_dict, batch_cam_info, delta_cam_info


    def perform_fitting(self, batch):
        B = self.img_crop_tensor.shape[0]
        # gt_img = (self.img_full_tensor[0].detach().cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)
        # gt_img = ((self.img_full_tensor[0].detach().cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255).astype(np.uint8)
        gt_img = ((self.img_crop_tensor[0].detach().cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255).astype(np.uint8)

        # Render
        with torch.no_grad():
            render_image = []
            bg_color = [1, 1, 1] if False else [0, 0, 0]
            background = torch.tensor(bg_color, dtype=torch.float32, device="cuda")
            N = self.gaussians.get_xyz.shape[0]
            for i in range(B):
                exp_coeffs = batch['base_code'][i, :, 80:144].squeeze().cuda() # (64)
                viewpoint_cam = Camera(colmap_id=0, R=(batch['cam_info']['batch_c2w_Rmats'][i].squeeze()).cpu().numpy(), T=(batch['cam_info']['batch_w2c_Tvecs'][i].squeeze()).cpu().numpy(),
                    FoVx=batch['cam_info']['FoVx'][i], FoVy=batch['cam_info']['FoVy'][i],
                    image=torch.zeros((3, 512, 512)), # gt_alpha_mask=loaded_mask,
                    image_name='oo', uid=0,
                    # data_device=args.data_device if not args.load2gpu_on_the_fly else 'cpu',
                    exp_coeffs=exp_coeffs,)
            
                exp_coeffs_input = exp_coeffs.unsqueeze(0).expand(N, -1)
                d_xyz, d_rotation, d_scaling = self.deform.step(self.gaussians.get_xyz.detach(), exp_coeffs_input)
            
                render_pkg_re = render(viewpoint_cam, self.gaussians, self.pipe, background, d_xyz, d_rotation, d_scaling, False)
                image, viewspace_point_tensor, visibility_filter, radii = render_pkg_re["render"], render_pkg_re[
                    "viewspace_points"], render_pkg_re["visibility_filter"], render_pkg_re["radii"]
                render_image.append(image)

        x = self.rest_img
        # y = self.img_crop_tensor.clone()
        # y[self.mask_tensor.repeat(1, 3, 1, 1) == 0.] = -1.
        # y = self.img_crop_mask_tensor
        y = torch.stack(render_image, dim=0) * 2 - 1
        # pred = self.netG(torch.cat([self.rest_img_full, y], dim=1))
        # pred = self.netG(torch.cat([x, y], dim=1))
        pred = self.netG(x, y)

        pred_dict = {'image': pred}
        batch_loss_dict = self.loss_utils.calc_total_loss(
            pred_dict=pred_dict, 
            gt_rgb=self.img_crop_tensor, mask_tensor=self.mask_tensor, netD=self.netD, lm68=self.lm68,
            rest_mask=self.rest_mask,
        )

        # coarse_fg_rgb_0 = self.rest_img_full
        coarse_fg_rgb_0 = x
        coarse_fg_rgb_1 = y
        coarse_fg_rgb_2 = pred_dict["image"]

        # coarse_fg_rgb_0 = (coarse_fg_rgb_0[0].detach().cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)
        # coarse_fg_rgb_1 = (coarse_fg_rgb_1[0].detach().cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)
        # coarse_fg_rgb_2 = (coarse_fg_rgb_2[0].detach().cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)

        coarse_fg_rgb_0 = ((coarse_fg_rgb_0[0].detach().cpu().permute(1, 2, 0) + 1) / 2 * 255).clamp(0, 255).numpy().astype(np.uint8)
        coarse_fg_rgb_1 = ((coarse_fg_rgb_1[0].detach().cpu().permute(1, 2, 0) + 1) / 2 * 255).clamp(0, 255).numpy().astype(np.uint8)
        coarse_fg_rgb_2 = ((coarse_fg_rgb_2[0].detach().cpu().permute(1, 2, 0) + 1) / 2 * 255).clamp(0, 255).numpy().astype(np.uint8)
        # cv2.imwrite("./temp_res/opt_imgs/img_%04d.png" % iter_, coarse_fg_rgb[:, :, ::-1])
        res_img = np.concatenate([coarse_fg_rgb_0, coarse_fg_rgb_1, coarse_fg_rgb_2, gt_img], axis=1)

        self.res_img = res_img
        # self.res_code_info = code_info
        # self.res_cam_info = cam_info
        
        return batch_loss_dict


    def save_res(self, base_name):
        # Generate Novel Views
        # render_nv_res = self.render_utils.render_novel_views(self.net, self.res_code_info)
        # NVRes_save_path = "%s/FittingResNovelView_%s.gif" % (save_root, base_name)
        # imageio.mimsave(NVRes_save_path, render_nv_res, 'GIF', duration=self.duration)
        
        # Generate Rendered FittingRes
        img_save_path = "%s/FittingRes_%s.png" % (self.opt.save_root, base_name)

        self.res_img = put_text_alignmentcenter(self.res_img, self.pred_img_size, "Input", (0, 0, 0), offset_x=0)
        self.res_img = put_text_alignmentcenter(self.res_img, self.pred_img_size, "Fitting", (0, 0, 0), offset_x=self.pred_img_size,)
        # self.res_img = cv2.putText(self.res_img, "Input", (110, 240), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1)
        # self.res_img = cv2.putText(self.res_img, "Fitting", (360, 240), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1)
        cv2.imwrite(img_save_path, self.res_img[:, :, ::-1])

        # for k, v in self.res_code_info.items():
        #     if isinstance(v, torch.Tensor):
        #         self.res_code_info[k] = v.detach()
        
        # temp_dict = {
        #     "code": self.res_code_info
        # }

        # torch.save(temp_dict, "%s/LatentCodes_%s_%s.pth" % (save_root, base_name, self.model_name))

    def save_networks(self, epoch):
        save_filename = 'epoch_%s.pth' % (epoch)
        save_path = "%s/%s" % (self.opt.save_root, save_filename)

        torch.save({'net': self.netG.state_dict(),
                    'para': {"featmap_size": 32,
                            "featmap_nc": 256,
                            "pred_img_size": 512,},
                    'optimizer_states': self.optimizer.state_dict(),
                    'lr_schedulers': self.scheduler.state_dict()}, save_path)
        
        if self.netD is not None:
            save_filename = 'epoch_D_%s.pth' % (epoch)
            save_path = "%s/%s" % (self.opt.save_root, save_filename)

            torch.save({'net': self.netD.state_dict(),
                    'optimizer_states': self.optimizer_D.state_dict(),
                    'lr_schedulers': self.scheduler_D.state_dict()}, save_path)

    def fitting_single_images(self, training_set):
        init_learn_rate = 0.001
    
        step_decay = 50

        params_group = [
            {'params': self.netG.parameters(), 'lr': init_learn_rate * 1.0},
        ]

        self.optimizer = torch.optim.Adam(params_group, betas=(0.9, 0.999))
        lr_func = lambda epoch: 0.1 ** (epoch / step_decay)
        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lr_func)

        params_group = [
            {'params': self.netD.parameters(), 'lr': init_learn_rate * 1.0},
        ]
        
        self.optimizer_D = torch.optim.Adam(params_group, betas=(0.9, 0.999))
        lr_func = lambda epoch: 0.1 ** (epoch / step_decay)
        self.scheduler_D = torch.optim.lr_scheduler.LambdaLR(self.optimizer_D, lr_lambda=lr_func)

        loop_bar = trange(self.opt.max_epochs)
        for i in loop_bar:
            for j, batch in enumerate(training_set):
                self.load_data(batch)

                batch_loss_dict = self.perform_fitting(batch)

                self.optimizer.zero_grad()
                batch_loss_dict["total_loss"].backward()
                self.optimizer.step()

                if self.netD is not None:
                    self.optimizer_D.zero_grad()
                    batch_loss_dict["loss_D"].backward()
                    self.optimizer_D.step()

                    loop_bar.set_description("Opt, Loss: %.6f D_fake: %.6f D_real: %.6f " % (
                            batch_loss_dict["head_loss"].item(), batch_loss_dict["D_fake"].item(), batch_loss_dict["D_real"].item()))
                else:
                    # loop_bar.set_description("Opt, Loss: %.6f" % (
                    #         batch_loss_dict["head_loss"].item()))
                    loop_bar.set_description("Opt, Loss: %.6f expr: %.6f appea: %.6f" % (
                            batch_loss_dict["head_loss"].item(), batch_loss_dict['expr'].item(), 
                            batch_loss_dict['appea'].item()))
                if j % 200 == 0:
                    self.save_res("train_%d_%d" % (i, j))

            self.save_networks(i)
            self.scheduler.step()
            if self.netD is not None:
                self.scheduler_D.step()
        
        
if __name__ == "__main__":
    torch.manual_seed(45) # cpu
    torch.cuda.manual_seed(55) # gpu
    np.random.seed(65) # numpy
    random.seed(75) # random and transforms

    parser = argparse.ArgumentParser(description='a framework for fitting a single image using HeadNeRF')
    lp = ModelParams(parser)
    pp = PipelineParams(parser)
    # parser.add_argument("--model_path", type=str)
    parser.add_argument("--txt_path", type=str)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--max_epochs", type=int, default=50)
    parser.add_argument("--save_root", type=str, required=True)
    parser.add_argument("--clip_length", type=int, default=1)
    parser.add_argument('--cfg', type=str, default='./config/face_3dmm_expression_mouth_mask.yaml', help='the config file path')
    
    args = parser.parse_args()
    # args = TestOptions().parse(parser)

    opt = OmegaConf.load(args.cfg)
    opt.update(vars(args))
    
    tt = FittingImage(lp.extract(args), pp.extract(args), opt, gpu_id=0)
    training_set = DataLoader(opt).load_data()
    tt.fitting_single_images(training_set)
